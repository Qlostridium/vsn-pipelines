# SingleCellTxBenchmark

A repository of pipelines for single-cell data in Nextflow DSL2.

All the output generated by the pipelines will be located in the directory specified by `params.global.outdir` in `nextflow.config`.

# Dependencies

Make sure you have the following softwares installed,
- nextflow (version 19.09.0-edge)
* A container system, either of:
    * [Docker](https://docs.docker.com/)
    * [Singularity](https://www.sylabs.io/singularity/)

# Quick start

To run a quick test of the single sample analysis pipeline, we can use the 1k PBMC datasets provided by 10x Genomics.
This will take only **~3min** to run.
The data first needs to be downloaded.
Instructions for downloading the test dataset can be found [here](https://github.com/aertslab/SingleCellTxBenchmark/tree/master/data).

Next, a config file needs to be generated.
In your working directory, run `nextflow config ...` with the appropriate profiles:
```bash
nextflow config aertslab/SingleCellTxBenchmark \
    -profile singularity,single_sample > single_sample.config
```
Now, edit `single_sample.config`.
Most of the default values are already set for the test dataset, but certain variables (e.g. container links) may need to be changed.
In particular, `params.global.tenx_folder` should point to the `outs/` folder in the 10x data, and
    `params.sc.file_converter` should be a path to the sample metadata file.

Now, the pipeline can be run using the config file just generated (`-C ...`), and specifying the `single_sample` workflow as an entrypoint:
```bash
nextflow -C single_sample.config \
   run aertslab/SingleCellTxBenchmark \
      -entry single_sample
```

The pipelines will generate 3 types of results in the output directory (`params.global.outdir`) 
- `data`: contains all the intermediate files.
- `loom`: contains final loom files which can be imported inside SCope visualization tool for further insight of the results.
- `notebooks`: contains all the notebooks generated along the pipeline (e.g.: Quality control report)
    - See the example output report from the 1k PBMC data 
      [here](notebooks/10x_PBMC.merged_report.html).
- `pipeline_reports` (if `-profile report` was passed to `nextflow config ...`)

If you would like to use the pipelines on a custom dataset, please go to the `Pipelines` section (see below).

# Pipelines

## General workflow and strategy
### Running the pipeline directly from GitHub:

```bash
nextflow run aertslab/SingleCellTxBenchmark \
    -profile singularity,single_sample \
    -entry single_sample
```
This picks up `aertslab/SingleCellTxBenchmark/main.nf` and runs the workflow defined by the `-entry` setting (here, `single_sample`), using the built-in configs, which are merged from each tool used (defined in the `single_sample` profile).
Specifying `nextflow run -latest ...` will download the latest commit prior to execution, or the `-r ...` option can be used to specify a specific commit or branch.

### Customizing config files
#### Changing parameters in a single config file
In order to use a customized config file, one of the tool-specific default files can be used as a template, then passed on to nextflow at run time. 
For example:
```bash
wget https://raw.githubusercontent.com/aertslab/SingleCellTxBenchmark/master/src/scanpy/scanpy.config
# edit the config file parameters, then:
nextflow -c scanpy.config \
   run aertslab/SingleCellTxBenchmark \
      -profile singularity,bbknn \
      -user <GitHub-user>
```
However, note that it is not possible to pass a custom `nextflow.config` directly.

#### Generating a custom config file using `nextflow config`
The preferred method is now to first run `nextflow config ...` to generate a template config file in your working directory.
The tool-specific parameters, as well as Docker/Singularity profiles, are included when specifying the appropriate profiles to `nextflow config`.
Any of the parameters in this config file can then be edited and used to run the workflow of your choice.
For example, to run the `single_sample` workflow in a new working directory:

* Generate the config using the `single_sample` and `singularity` profiles:
```bash
mkdir single_sample_test && cd single_sample_test

nextflow config aertslab/SingleCellTxBenchmark \
    -profile singularity,single_sample > single_sample.config
```
* Now run the workflow using the new config file (using `-C` to use only this file), specifying the proper workflow as the entry point:
```bash
nextflow -C single_sample.config \
   run aertslab/SingleCellTxBenchmark \
      -entry single_sample
```

## Single-sample Datasets

Pipelines to run a single sample.

The **single_sample** workflow will process 10x data,taking in 10x-structured data, and metadata file.


## Multiple Datasets

Pipelines to aggregate multiple datasets together.

### BBKNN 
Source: https://github.com/Teichlab/bbknn/blob/master/examples/pancreas.ipynb

**How to run on 10xGenomics datasets ?**

```{bash}
OUTPUT_DIRECTORY="out"
PROJECT_NAME="tiny"
```

Let's say the file structure of your data looks like this,

```
/home/data/
└── cellranger
    ├── Sample A
    │   └── outs
    │       ├── filtered_feature_bc_matrix
    │       └── ...
    └── Sample_B
        └── outs
            ├── filtered_feature_bc_matrix
            └── ...
```

Then the command to run the pipeline will be:

Using singularity,
```{bash}
nextflow run \
   src/scanpy/bec_bbknn.nf \
      -profile singularity \
      --tenx_folder /home/data/cellranger/**/filtered_feature_bc_matrix \
      --sample_metadata /home/data/cellranger/metadata.tsv \
      --outdir ${OUTPUT_DIRECTORY} \
      --project_name ${PROJECT_NAME} \
      -with-report report.html \
      -with-trace
```

# Repository structure

## Root
The repository root contains a `main.nf` and associated `nextflow.config`.
The root `main.nf` imports and calls sub-workflows defined in the modules.

## Modules
A "module" consists of a folder labeled with the tool name (Scanpy, SCENIC, utils, etc.), with subfolders for
* `bin/` (scripts passed into the container)
* `processes/` (where Nextflow processes are defined)
The root of the modules folder contains workflow files + associated configs (as many as there are workflows):
* `main.nf` + `nextflow.config`
* `single_sample.nf` + `scenic.config`
* ...

```
src/
├── cellranger
│   ├── main.nf
│   ├── nextflow.config
│   └── processes
│       ├── count.nf
│       └── mkfastq.nf
│
├── channels
│   └── tenx.nf
│
├── scenic
│   ├── bin
│   │   ├── grnboost2_without_dask.py
│   │   └── merge_SCENIC_motif_track_loom.py
│   ├── processes
│   │   ├── aucell.nf
│   │   ├── cistarget.nf
│   │   ├── grnboost2withoutDask.nf
│   │   └── mergeScenicLooms.nf
│   ├── main.nf
│   └── scenic.config
│
└── utils
    ├── bin
    │   ├── h5ad_to_loom.py
    │   ├── sc_file_annotator.py
    │   ├── sc_file_concatenator.py
    │   └── sc_file_converter.py
    ├── utils.config
    └── processes
        ├── files.nf
        ├── h5ad_to_loom.nf
        ├── utils_1.test.nf
        ├── utils_2.test.nf
        └── utils.nf
```

## Workflows

Workflows (chains of nf processes) are defined in the module root folder (e.g. [src/Scanpy/bec_bbknn.nf](https://github.com/aertslab/SingleCellTxBenchmark/blob/module_refactor/src/scanpy/bec_bbknn.nf))
Workflows import multiple processes and define the workflow by name:
```groovy
include SC__CELLRANGER__MKFASTQ from './processes/mkfastq'  params(params)
include SC__CELLRANGER__COUNT   from './processes/count'    params(params)

workflow CELLRANGER {
    main:
        SC__CELLRANGER__MKFASTQ(file(params.sc.cellranger.mkfastq.csv), file(params.sc.cellranger.mkfastq.runFolder))
        SC__CELLRANGER__COUNT(file(params.sc.cellranger.count.transcriptome), SC__CELLRANGER__MKFASTQ.out.flatten())
    emit:
        SC__CELLRANGER__COUNT.out
}

```

### Workflow imports
Entire **sub-workflows** can also be imported in other workflows with one command (inheriting all of the process imports from the workflow definition):
```groovy
include CELLRANGER from '../cellranger/main.nf' params(params)
```

This leads to the ability to easily define **high-level workflows** in the master nf file: `aertslab/SingleCellTxBenchmark/main.nf`:
```groovy
include CELLRANGER from './src/cellranger/main.nf' params(params)
include BEC_BBKNN from './src/scanpy/bec_bbknn.nf' params(params)
include SCENIC from './src/scenic/main.nf' params(params)

workflow {
    CELLRANGER()
    BEC_BBKNN( CELLRANGER.out )
    SCENIC( BEC_BBKNN.out )
}
```


## Parameters structure
Parameters are stored in a separate config file per workflow, plus the main `nextflow.config`. 
These parameters are merged when starting the run using e.g.:
```groovy
includeConfig 'src/scenic/nextflow.config'
```

The parameter structure internally (post-merge) is:
```groovy
params {
    global {
        baseFilePath = "/opt/SingleCellTxBenchmark"
        project_name = "MCF7"
        ...
    }
    sc {
        utils {
            file_converter {
                ...
            }
            file_annotator {
                ...
            }
            file_concatenator {
                ...
            }
        }
        scanpy {
            container = 'docker://dweemx/sctx-scanpy:0.2.0'
            filter {
                ...
            }
            data_transformation {
                ...
            }
            normalization {
                ...
            }
            feature_selection {
                ...
            }
            feature_scaling {
                ...
            }
            dim_reduction {
                pca {
                    dimReductionMethod = 'PCA' 
                    ...
                }
                umap {
                    dimReductionMethod = 'UMAP' 
                    ...
                }
            }
            batch_effect_correct {
                ...
            }
            clustering {
                ...
            }
        }
    }
}

```

# Development

## Module testing

Modules and processes can be tested independently, you can find an example in `src/utils/main.test.nf`.

The `SC__FILE_CONVERTER` process is tested against the `tiny` dataset available in `data/01.count`.

